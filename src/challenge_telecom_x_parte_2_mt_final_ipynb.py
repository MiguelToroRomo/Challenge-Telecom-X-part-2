# -*- coding: utf-8 -*-
"""Challenge Telecom X: Parte 2 MT final.ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ltDImwv519yo-bcfjo5RHPZgGc19okNX

# üéØ<font color=blue size=6>Challenge Telecom X: An√°lisis de evasi√≥n de clientes - Parte 2</font>

<p align="center">
  <img src="https://drive.google.com/uc?export=view&id=1l5Ux2MaZCnJ7gHQSGb_vhqE_KR66PGio"
       alt="Imagen de portada"
       width="600" />
</p>

Autor: Miguel √Ångel Toro Romo

Fecha: 08-08-2025

---

üéØ Misi√≥n

El objetivo es tener un modelo predictivo capaz de prever qu√© clientes tienen mayor probabilidad de cancelar sus servicios.

La empresa quiere anticiparse al problema de la cancelaci√≥n, y debemos construir un pipeline robusto para esta etapa inicial de modelado.

üß† <font color=red size=4>Este es un problema de clasificaci√≥n binaria.</font>

üî∑ Variable objetivo binaria (Churn = 1 o 0):
¬øEl cliente se ir√° o no?


‚úÖ Modelos recomendados: Modelos de clasificaci√≥n.

Probaremos:

*   DecisionTreeClassifier
*   RandomForestClassifier
*   CatBoost

---


üß† Principales tareas del Desaf√≠o

‚úÖPreparar los datos para el modelado (tratamiento, codificaci√≥n, normalizaci√≥n).

‚úÖRealizar an√°lisis de correlaci√≥n y selecci√≥n de variables.

‚úÖEntrenar dos o m√°s modelos de clasificaci√≥n.

‚úÖEvaluar el rendimiento de los modelos con m√©tricas.

‚úÖInterpretar los resultados, incluyendo la importancia de las variables.

‚úÖCrear una conclusi√≥n estrat√©gica se√±alando los principales factores que influyen en la cancelaci√≥n.


---

### ‚öôÔ∏è<font color=blue size=5>Bibliotecas utilizadas</font>
"""

# Bibliotecas utilizadas
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import joblib
import pickle
import random
import warnings
warnings.filterwarnings('ignore')

from IPython.display import HTML, display

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (classification_report, confusion_matrix, roc_curve,
                             roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay,
                             recall_score,accuracy_score, precision_score, make_scorer,f1_score)

from sklearn.model_selection import (train_test_split, GridSearchCV, StratifiedKFold, KFold,
                                     cross_validate, RandomizedSearchCV, cross_val_score)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import resample
from statsmodels.stats.outliers_influence import variance_inflation_factor

from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
from imblearn.under_sampling import TomekLinks
from imblearn.pipeline import Pipeline as imbpipeline

from scipy import stats
from scipy.stats import t

# importamos catboost
!pip install catboost
from catboost import CatBoostClassifier

"""## üîß<font color=blue size=5>Funciones</font>

### Impresi√≥n personalizada
"""

# funci√≥n para impresiones con caracter√≠sticas especiales
def big_print(text, size=20, color="black"):
    print("\n")
    display(HTML(f"<span style='font-size: {size}px; color: {color};'>{text}</span>"))

"""### Generaci√≥n datos nuevos"""

# funci√≥n para generar datos de prueba nuevos con caracter√≠sticas aleatorias
# con tenure entre 0 y 6, ya que son los meses de mayor deserci√≥n
def generar_registro_cliente():
    tenure = random.randint(0, 6)
    Charges_Monthly = 65
    Charges_Total = tenure * Charges_Monthly

    return {
        'customerID': f'{random.randint(1000, 9999)}-XYZ',
        'gender': random.choice(['Male', 'Female']),
        'SeniorCitizen': random.choice([0, 1]),
        'Partner': random.choice(['Yes', 'No']),
        'Dependents': random.choice(['Yes', 'No']),
        'tenure': tenure,
        'PhoneService': random.choice(['Yes', 'No']),
        'MultipleLines': random.choice(['Yes', 'No', 'No phone service']),
        'InternetService': random.choice(['DSL', 'Fiber optic', 'No']),
        'OnlineSecurity': random.choice(['Yes', 'No', 'No internet service']),
        'OnlineBackup': random.choice(['Yes', 'No', 'No internet service']),
        'DeviceProtection': random.choice(['Yes', 'No', 'No internet service']),
        'TechSupport': random.choice(['Yes', 'No', 'No internet service']),
        'StreamingTV': random.choice(['Yes', 'No', 'No internet service']),
        'StreamingMovies': random.choice(['Yes', 'No', 'No internet service']),
        'Contract': random.choice(['Month-to-month', 'One year', 'Two year']),
        'PaperlessBilling': random.choice(['Yes', 'No']),
        'PaymentMethod': random.choice([
            'Electronic check', 'Mailed check',
            'Bank transfer (automatic)', 'Credit card (automatic)'
        ]),
        'Charges.Monthly': Charges_Monthly,
        'Charges.Total': Charges_Total
    }

"""### mapeo de columnas binarias"""

# funci√≥n para mapear columnas Yes y No
# Transforma columnas categ√≥ricas que contienen 'Yes' a variables binarias:
#    'Yes' -> 1, todo lo dem√°s -> 0.
#   Par√°metro:
#        df (pd.DataFrame): DataFrame original.
#    Retorna:
#       pd.DataFrame: DataFrame con las columnas transformadas.
def map_yes_to_binary(df):

    df_copy = df.copy()

    for col in df_copy.columns:
        if df_copy[col].dtype == 'object':
            unique_vals = df_copy[col].dropna().unique()
            if 'Yes' in unique_vals:
                # Mapea 'Yes' a 1, todo lo dem√°s a 0
                df_copy[col] = df_copy[col].apply(lambda x: 1 if x == 'Yes' else 0)

    return df_copy

"""### Despliegue de matriz de confusi√≥n"""

# funci√≥n para mostrar matriz de confusi√≥n
def plot_confusion_matrix(y_test, y_previsto, modelo:str):
  # creamos la matriz de confusi√≥n
  cm = confusion_matrix(y_test, y_previsto)

  # creamos el gr√°fico de la matriz de confusi√≥n
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Churn No', 'Churn S√≠'])
  disp.plot(cmap='Blues')
  plt.title(f'Matriz de Confusi√≥n {modelo}', color='firebrick', fontsize=12)

  # guadamos gr√°fico como archivo png
  archivo_matriz_confusion = str(f'MC_{modelo}_1.png')
  plt.savefig(archivo_matriz_confusion, dpi=300, bbox_inches='tight')
  plt.grid(False)
  plt.show()

"""### Despliegue de curva ROC"""

# Funci√≥n para mostrar gr√°fico de curva ROC
def plot_roc_curve(modelo, X_test, y_test, titulo):

  # revisemos la curva AUC para este modelo
  modelo.fit(X_train, y_train)

  # Predecir probabilidades sobre conjunto de prueba
  y_scores = modelo.predict_proba(X_test)[:, 1]  # Probabilidad de clase 1

  # Calcular fpr, tpr y auc
  fpr, tpr, thresholds = roc_curve(y_test, y_scores)
  auc = roc_auc_score(y_test, y_scores)

  # configuramos display del gr√°fico
  plt.figure(figsize=(6, 5), facecolor='lightyellow')
  ax = plt.gca()
  ax.set_facecolor('honeydew')

  plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})', linewidth=2)
  plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
  plt.xlabel('Tasa de Falsos Positivos (FPR)')
  plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
  plt.title(f'Curva ROC - {titulo}',fontsize=14)
  plt.legend(loc='lower right')
  plt.grid(True)
  plt.tight_layout()

  # guardamos gr√°fico como archivo png
# reemplazamos ' ' por '_' en titulo
  titulo = titulo.replace(' ', '_')
  archivo_roc = str(f'ROC_{titulo}.png')
  plt.savefig(archivo_roc, dpi=300, bbox_inches='tight')
  plt.show()

"""### Recopilaci√≥n de m√©tricas de modelos"""

# Lista para almacenar los resultados
metricas_modelos = []

# Funci√≥n para calcular m√©tricas
def calcular_metricas(modelo_nombre, y_true, y_pred):
    return {
        'Modelo': modelo_nombre,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision_Clase_1': precision_score(y_true, y_pred, pos_label=1),
        'Recall_Clase_1': recall_score(y_true, y_pred, pos_label=1),
        'F1-Score_Clase_1': f1_score(y_true, y_pred, pos_label=1)
    }

"""## <font color=blue size=5>Extracci√≥n del Archivo Normalizado del Challenge anterior</font>

---

En el challenge anterior terminamos con un dataset normalizado, donde se hab√≠an eliminado registros con valores nulos.

Utilizaremos ese dataset para este challenge.


---
"""

# leemos archivo /content/df_normalizado.csv
df = pd.read_csv('/content/df_normalizado.csv')

df.info()

df.head()

# imprimimos valores √∫nicos de las columnas del df
for col in df.columns:
  print(f'{col}: {df[col].unique()}')



"""## <font color=blue size=5>An√°lisis exploratorio de datos</font>

###<font color=blue size=5>Verificaci√≥n de la Proporci√≥n de Cancelaci√≥n (Churn)</font>
"""

# An√°lisis de distribuci√≥n de clases Churn Yes y Churn No
# creamos gr√°fico de torta con porcentajes y tabla con valores absolutos
sns.set(style="whitegrid")

# Contamos los valores
valores = df['Churn'].value_counts()
etiquetas = ['Churn No', 'Churn S√≠']
porcentajes = valores.values

# Creamos la figura y el eje
fig, ax = plt.subplots(figsize=(6, 6))

# Personalizamos colores de fondo y √°rea de gr√°fico
fig.patch.set_facecolor('lightyellow')   # Fondo de toda la figura
ax.set_facecolor('honeydew')             # Fondo del √°rea del gr√°fico

# Gr√°fico de torta
wedges, texts, autotexts = ax.pie(
    porcentajes,
    labels=etiquetas,
    autopct='%1.1f%%',
    startangle=90
)
ax.axis('equal')  # C√≠rculo perfecto

# Agregamos tabla con valores absolutos
tabla_data = [[etiquetas[i], porcentajes[i]] for i in range(len(etiquetas))]
col_labels = ['Categor√≠a', 'Cantidad']

tabla = ax.table(
    cellText=tabla_data,
    colLabels=col_labels,
    cellLoc='center',
    loc='bottom',
    bbox=[1.0, 0.3, 0.5, 0.2]  # [x, y, width, height]
)

tabla.auto_set_font_size(False)
tabla.set_fontsize(10)
plt.title('Distribuci√≥n de Churn')
plt.subplots_adjust(bottom=0.3)  # Espacio para la tabla

# guardamos gr√°fico como imagen png
plt.savefig('graf_01_torta_churn.png', dpi=300, bbox_inches='tight')

plt.show()

"""###<font color=blue size=5>Distribuci√≥n de Churn Yes por tenure</font>"""

# An√°lisis de Churn por tenure
# Filtramos clientes con Churn = Yes y tenure ‚â§ 12
churned_short_tenure = df[(df['Churn'] == 'Yes') & (df['tenure'] <= 12)]

# Contamos clientes por tenure
tenure_counts = churned_short_tenure['tenure'].value_counts().sort_index().reset_index()
tenure_counts.columns = ['Tenure', 'Clientes']

# Configuramos estilos
plt.style.use('seaborn-v0_8-whitegrid')

# Creamos figura con fondo personalizado
fig = plt.figure(figsize=(8, 5), facecolor='lightyellow')
ax = fig.add_subplot(facecolor='honeydew')  # Fondo del √°rea del gr√°fico

# Colores para las barras usando un gradiente
colors = [plt.cm.Reds(i) for i in np.linspace(0.4, 0.9, len(tenure_counts))]

# Creamos gr√°fico de barras
bars = ax.bar(tenure_counts['Tenure'],
              tenure_counts['Clientes'],
              color=colors,
              edgecolor='#333333',
              linewidth=1.2,
              alpha=0.9)

# A√±adimos etiquetas de valores
for bar in bars:
    height = bar.get_height()
    ax.annotate(f'{height}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),
                textcoords="offset points",
                ha='center',
                va='bottom',
                fontsize=10,
                fontweight='bold')

# Personalizaci√≥n del gr√°fico
ax.set_title('Clientes que abandonaron (Churn=Yes) con ‚â§ 12 meses de permanencia',
             fontsize=16, pad=20, fontweight='bold', color='#333333')
ax.set_xlabel('Meses de permanencia (Tenure)', fontsize=12, labelpad=10, color='#333333')
ax.set_ylabel('Clientes', fontsize=12, labelpad=10, color='#333333')
ax.set_xticks(range(0, 13))
ax.tick_params(axis='both', colors='#555555')

# Decoraci√≥n adicional
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_color('#cccccc')
ax.spines['bottom'].set_color('#cccccc')

# A√±adimos nota explicativa
plt.figtext(0.5, -0.05,
            'An√°lisis de clientes que abandonaron durante los primeros 12 meses',
            ha='center', fontsize=10, color='#666666', fontstyle='italic')
plt.grid(False)

# Ajustar layout y mostrar
plt.tight_layout()
plt.subplots_adjust(bottom=0.12)  # Espacio para el texto inferior

# guardamos gr√°fico como archivo png
plt.savefig('graf_02_desercion_primer_a√±o.png', dpi=300, bbox_inches='tight')
plt.show()

"""###<font color=blue size=5>Categor√≠as con mayor porcentaje de Churn Yes</font>"""

# Analizamos las caracter√≠sticas con mayor Churn Yes
# Convertir Churn a binario (1 para Yes, 0 para No)
df['Churn_bin'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

# Lista de caracter√≠sticas categ√≥ricas
categorical_features = [
    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',
    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
    'Contract', 'PaperlessBilling', 'PaymentMethod'
]

# Calcular el porcentaje de churn para cada categor√≠a
results = []
for feature in categorical_features:
    # Convertir a string para incluir SeniorCitizen correctamente
    df[feature] = df[feature].astype(str)
    grouped = df.groupby(feature)['Churn_bin'].mean().reset_index()
    grouped['Feature'] = feature
    # Renombrar la columna de categor√≠a para unificar
    grouped.rename(columns={feature: 'Category'}, inplace=True)
    results.append(grouped)

# Combinar todos los resultados
results_df = pd.concat(results, ignore_index=True)
results_df.rename(columns={'Churn_bin': 'Churn_Percentage'}, inplace=True)

# Ordenar y seleccionar las top 10 categor√≠as con mayor churn
top_10 = results_df.sort_values('Churn_Percentage', ascending=False).head(10)

# Ordenar para mejor visualizaci√≥n en el gr√°fico
top_10 = top_10.sort_values('Churn_Percentage', ascending=True)

# Crear etiquetas combinadas
top_10['Label'] = top_10['Feature'] + ': ' + top_10['Category']

# Creamos figura con fondo personalizado
fig = plt.figure(figsize=(8, 5), facecolor='lightyellow')
ax = fig.add_subplot(facecolor='honeydew')  # Fondo del √°rea del gr√°fico

# Crear colores con gradiente
colors = plt.cm.Greens(np.linspace(0.4, 0.9, len(top_10)))

# Gr√°fico de barras horizontales
bars = ax.barh(top_10['Label'],
               top_10['Churn_Percentage'] * 100,
               color=colors,
               edgecolor='#333333',
               height=0.8)

# A√±adir valores a las barras
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.5,
             bar.get_y() + bar.get_height()/2,
             f'{width:.1f}%',
             va='center',
             fontsize=10,
             fontweight='bold')

# Personalizaci√≥n
plt.title('Top 10 Categor√≠as con Mayor Porcentaje de Churn',
          fontsize=16, pad=20, fontweight='bold', color='#333333')
plt.xlabel('Porcentaje de Churn (%)', fontsize=12, labelpad=10, color='#333333')
plt.ylabel('Categor√≠as', fontsize=12, labelpad=10, color='#333333')

# AJUSTE PRINCIPAL: Limitar eje X al 60%
plt.xlim(0, 70)
plt.xticks(np.arange(0, 61, 5))

# A√±adir l√≠nea vertical de referencia
avg_churn = df['Churn_bin'].mean() * 100
plt.axvline(x=avg_churn,
            color='#3498db',
            linestyle='--',
            linewidth=1.5,
            label=f'Churn Promedio ({avg_churn:.1f}%)')

# Destacar la barra con mayor churn
max_bar = bars[-1]
max_bar.set_edgecolor('#8b0000')
max_bar.set_linewidth(2)

# Decoraci√≥n
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_color('#cccccc')
ax.spines['bottom'].set_color('#cccccc')
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.legend(loc='lower right')

plt.tight_layout()
plt.subplots_adjust(bottom=0.1)
plt.grid(False)

# guardamos gr√°fico como archivo png
plt.savefig('graf_03_top_10_churn.png', dpi=300, bbox_inches='tight')

plt.show()

# eliminamos Churn_bin
df.drop('Churn_bin', axis=1, inplace=True)

"""# üöß<font color=blue size=6>Preparaci√≥n de datos</font>

## <font color=blue size=5>Eliminaci√≥n de Columnas Irrelevantes</font>

---
<font size=4>

*   **customerID**: es  una columna generada para identificaci√≥n de los registros, por lo que no representa informaci√≥n de las caracter√≠sticas de los clientes.</font>
---
"""

# eliminamos columna customerID
df.drop('customerID', axis=1, inplace=True)
df.head(5)

"""## <font color=blue size=5>Encoding</font>

<font size=4>

*   Para la codificaci√≥n reemplazaremos los Yes por 1 y los No por 0.

*   Para las columnas categ√≥ricas restantes utilizaremos OneHotEncoder
</font>
"""

# mapeamos los Yes a 1, No y No+cualquier_cosa a 0
df = map_yes_to_binary(df)

df.head()

# Separamos caracter√≠sticas (X) y variable objetivo (y)
X = df.drop(columns=['Churn'])
y = df['Churn']

# Identificamos columnas categ√≥ricas restantes a codificar
categorical_cols = ['gender', 'InternetService', 'Contract', 'PaymentMethod']

# Configuramos el transformador
preprocessor = ColumnTransformer(transformers=[
                                ('cat',
                                 OneHotEncoder(drop='first', sparse_output=False),
                                 categorical_cols)],
                                  remainder='passthrough')

# Aplicamos el encoded a las caracter√≠sticas
X_encoded = preprocessor.fit_transform(X)

# Convertimos a DataFrame con nombres de columnas
# Obtenemos nombres de caracter√≠sticas despu√©s del encoding
feature_names = preprocessor.get_feature_names_out()
X_encoded_df = pd.DataFrame(X_encoded, columns=feature_names)
X_encoded_df.head()

# generamos dataframe completo con X_encoded_df + y
df_codificado = pd.concat([X_encoded_df, y], axis=1)
df_codificado.head()

"""<font size=4>Existe una correlaci√≥n fuerte entre **Charges.Monthly** y **Charges.Total**, probablemente porque existe una funci√≥n entre ellas y el valor de tenure.
Por lo tanto, parece conveniente eliminar Charges.Monthly para simplificar el dataset.</font>

"""

# eliminamos remainder__Charges.Monthly
df_codificado.drop('remainder__Charges.Monthly', axis=1, inplace=True)
df_codificado.head()

# guardamos dataframe df_codificado como archivo csv
df_codificado.to_csv('df_codificado.csv', index=False)

"""<font color=red size=4>A partir de ahora trabajaremos con el dataframe codificado guardado</font>

##<font color=blue size=5>An√°lisis de correlaci√≥n de variables</font>
"""

# Analizamos la correlaci√≥n de las variables con Churn, filtrando un umbral de correlaci√≥n en 15%
df = pd.read_csv('/content/df_codificado.csv')
umbral = 0.15

# Matriz de correlaci√≥n
corr_matrix = df.corr()

# Obtenemos correlaciones con 'Churn'
correlaciones_churn = corr_matrix['Churn'].drop('Churn')

# Filtramos por umbral
correlaciones_filtradas = correlaciones_churn[abs(correlaciones_churn) > umbral].sort_values(ascending=False)

# Mostramos tabla de correlaciones ordenada
big_print(f"Correlaciones con 'Churn' mayores a {umbral}%",20,'brown')
display(correlaciones_filtradas.to_frame(name='Correlaci√≥n'))

# Creamos submatriz para heatmap
variables = ['Churn'] + list(correlaciones_filtradas.index)
filtered_corr_matrix = corr_matrix.loc[variables, variables]

# Creamos m√°scara inferior
mask = np.triu(np.ones_like(filtered_corr_matrix, dtype=bool))

# Plot del heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(filtered_corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title(f"Mapa de calor: Correlaciones con 'Churn' > {umbral}",fontsize=14)
plt.xticks(rotation=90, ha='right')
plt.tight_layout()
plt.grid(False)

# guardamos mapa de calor como archivo png
plt.savefig('mapa_de_calor_correlacion_churn.png', dpi=300, bbox_inches='tight')
plt.show()

"""##<font color=blue size=5>An√°lisis de multicolinealidad</font>

---





*   <font size=4>Los modelos a probar no son sensibles a la multicolinealidad, por lo tanto omitiremos este an√°lisis ya que no aporta para la elecci√≥n del modelo apropiado para este problema.</font>





---

# üì¶<font color=blue size=6>Modelos probados</font>

Probaremos:

*   DecisionTreeClassifier
*   RandomForestClassifier
*   CatBoost

<font color=red size=4>
Recordemos las f√≥rmulas para el c√°lculo de las m√©tricas de los modelos de clasificaci√≥n</font>

<img src="https://i.ibb.co/6Jp6rKyC/Captura-de-tela-2025-03-22-120744.png" width="400">

## <font color=blue size=5>DecisionTreeClassifier</font>

### <font color=blue size=5>DecisionTreeClassifier sin optimizaci0nes</font>
"""

# Separamos variables dependiente e independientes
# leemos archivo guardado
df = pd.read_csv('df_codificado.csv')

X = df.drop('Churn', axis=1)
y = df['Churn']

# Separamos Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# creamos modelo DecisionTreeClasifier
modelo = DecisionTreeClassifier(random_state=42,max_depth=10)

# entrenamos con conjunto de train
modelo.fit(X_train, y_train)
# revisemos las m√©tricas y la matriz de confusi√≥n

y_previsto = modelo.predict(X_test)

big_print("Reporte de Clasificaci√≥n DecisionTree sin optimizaciones:", 20,'brown')
print(classification_report(y_test, y_previsto))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_previsto, 'DecisionTree sin optimizaciones')

"""<font color=red size=4>
¬øQu√© significa esta matriz de confusi√≥n?</font>

1. que predijo correctamente 899 Churn No (Verdaderos Negativos TN) **87%**

2. que predijo err√≥neamente 136 Churn No como Churn S√≠ (Falsos Positivos FP)

3. predijo correctamente 196 Churn S√≠ (Verdaderos Positivos TP) **48%**

4. predijo err√≥neamente 178 Churn S√≠ como Churn No (Falsos Negativos FN)

Esto nos muestra que el modelo es pr√°cticamente in√∫til para predecir correctamente los Churn S√≠ ya que solo identifica correctamente al **48%** de ellos a pesar de que identifica correctamente a casi al **87%** de los Churn No.

Este modelo no es √∫til porque nuestro objetivo es identificar lo mejor posible los clientes con Churn S√≠.

**Nuestro objetivo es encontrar un modelo donde el recall para Churn S√≠ sea el mayor posible.**


---
"""

# mostramos curva ROC
plot_roc_curve(modelo, X_test, y_test, 'DecisionTreeClassifier con RandomOverSampler')

"""###<font color=blue size=5> DecisionTreeClassifier con Validaci√≥n cruzada</font>"""

# Separamos variables predictoras (X) y variable objetivo (y)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separamos en conjunto de entrenamiento y prueba (stratify mantiene proporci√≥n de clases)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Creamos modelo
modelo = DecisionTreeClassifier(random_state=42, max_depth=10)

# Validaci√≥n cruzada sobre el conjunto de entrenamiento
scoring = ['accuracy', 'precision', 'recall', 'f1']
cv_resultados = cross_validate(
    modelo, X_train, y_train, cv=5, scoring=scoring, return_train_score=False
)

# Mostramos resultados promedios de la validaci√≥n cruzada
big_print("Validaci√≥n cruzada (solo en TRAIN)",20,'brown')
for metrica in scoring:
    puntajes = cv_resultados[f'test_{metrica}']
    print(f"{metrica.capitalize():<10}: {puntajes.mean():.3f} ¬± {puntajes.std():.3f}")

# Entrenamos el modelo final con todos los datos de entrenamiento
modelo.fit(X_train, y_train)

# Predecimos con datos de test (jam√°s vistos por el modelo)
y_pred = modelo.predict(X_test)

# Reporte de clasificaci√≥n
big_print("Reporte de Clasificaci√≥n DecisionTree con CrossValidation", 20,'firebrick')
print(classification_report(y_test, y_pred))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_pred, 'DecisionTree con validaci√≥n cruzada')

"""<font color=red size=4>
¬øQu√© significa esta matriz de confusi√≥n?</font>

1. que predijo correctamente 894 Churn No (Verdaderos Negativos TN) **86%**

2. que predijo err√≥neamente 141 Churn No como Churn S√≠ (Falsos Positivos FP)

3. predijo correctamente 184 Churn S√≠ (Verdaderos Positivos TP) **49%**

4. predijo err√≥neamente 190 Churn S√≠ como Churn No (Falsos Negativos FN)

<font color=red size=4>
¬øQu√© significan las m√©tricas obtenidas con a validaci√≥n cruzada?</font>



1.   **DecisionTreeClassifier** entrenado con validaci√≥n cruzada (CV) muestra las mismas m√©tricas que el modelo entrenado sin CV (usando un solo train-test split), esto nos indica:

  - Estabilidad del modelo: El modelo es robusto ante diferentes particiones de datos.

  - No es sensible a variaciones aleatorias en la divisi√≥n entrenamiento/prueba.

2. Calidad de los datos: Los datos est√°n bien distribuidos y no hay sesgos fuertes o patrones ocultos que afecten el rendimiento seg√∫n c√≥mo se dividan.

3. El tama√±o del dataset es adecuado (no es demasiado peque√±o), lo que reduce la variabilidad en las m√©tricas entre diferentes splits.

4. Ausencia de overfitting significativo
El modelo generaliza bien: su rendimiento en datos no vistos es consistente, independientemente de c√≥mo se eval√∫e (CV vs. holdout simple).

###<font color=blue size=5> DecisionTreeClassifier con balanceo de clases</font>

---


<font color=red size=4>Probaremos cuatro t√©cnicas de balanceo</font>


*   RandomOverSampler
*   SMOTE
*   ADASYN
*   TomekLinks


---
"""

# Cargamos datos
df = pd.read_csv("df_codificado.csv")

# Separamos variables
X = df.drop("Churn", axis=1)
y = df["Churn"]

# Separamos en train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Diccionario con t√©cnicas de muestreo
samplers = {
    "RandomOverSampler": RandomOverSampler(random_state=42),
    "SMOTE": SMOTE(random_state=42),
    "ADASYN": ADASYN(random_state=42),
    "TomekLinks": TomekLinks()
}

# Almacenar resultados
resultados = []

# Entrenamiento y evaluaci√≥n por t√©cnica
def entrenar_y_evaluar(X_res, y_res, nombre):
    modelo = DecisionTreeClassifier(max_depth=10, random_state=42)
    modelo.fit(X_res, y_res)
    y_pred = modelo.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    resultados.append({
        "T√©cnica": nombre,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    big_print(f"===== {nombre} =====",20,'brown')
    print(classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Churn No', 'Churn S√≠'])
    disp.plot(cmap="Blues")
    plt.title(f"Matriz de Confusi√≥n DecisionTree balanceado con {nombre}", color='firebrick', fontsize=12)
    plt.grid(False)
    # guardamos gr√°fico como archivo png
    plt.savefig(f'MC_DecisionTree_{nombre}.png', dpi=300, bbox_inches='tight')
    plt.show()

# Ejecutar para cada t√©cnica
for nombre, sampler in samplers.items():
    try:
        X_res, y_res = sampler.fit_resample(X_train, y_train)
        entrenar_y_evaluar(X_res, y_res, nombre)
    except Exception as e:
        print(f"Error en {nombre}: {e}")

# Mostrar tabla comparativa
df_resultados = pd.DataFrame(resultados)
big_print("üìä M√©tricas DecisionTree por T√©cnica de Balanceo:",20,'brown')
print(df_resultados.sort_values("Recall", ascending=False))

"""---


<font color=red size=4>Nuestro objetivo es minimizar los falsos negativos:</font>

Es decir, cuando el modelo predice que un cliente no har√° churn (0), pero en realidad s√≠ lo har√° (1).

El criterio que debemos priorizar es el **Recall** (sensibilidad) para la clase positiva (Churn = 1).

Con este criterio el mejor balanceo de clases lo provee **RandomOverSamplet**, aunque la Precision es la m√°s baja, pero en t√©rminos globales tiene tambi√©n el mejor F1-score.


---

<font color=red size=5>Graficamos Recall y F1 para las cuatro t√©cnicas de balanceo</font>
"""

# Ordenar el DataFrame por Recall de mayor a menor
df_sorted = df_resultados.sort_values('Recall', ascending=False)

# Configuraci√≥n del gr√°fico
plt.figure(figsize=(8, 4), facecolor='lightyellow')
ax = plt.gca()
ax.set_facecolor('honeydew')

# Configuraci√≥n de barras
n = len(df_sorted)
bar_width = 0.35
index = np.arange(n)

# Creamos barras para Recall y F1-score
plt.bar(index - bar_width/2, df_sorted['Recall'], bar_width,
        color='skyblue', label='Recall')
plt.bar(index + bar_width/2, df_sorted['F1-score'], bar_width,
        color='lightgreen', label='F1-score')

# A√±adimos valores en las barras
for i, (rec, f1) in enumerate(zip(df_sorted['Recall'], df_sorted['F1-score'])):
    plt.text(i - bar_width/2, rec + 0.02, f"{rec:.3f}",
             ha='center', fontsize=9, color='darkblue')
    plt.text(i + bar_width/2, f1 + 0.02, f"{f1:.3f}",
             ha='center', fontsize=9, color='darkred')

# Personalizaci√≥n del gr√°fico
plt.title('DecisionTree Recall y F1-Score por t√©cnica de balanceo', pad=20, fontsize=14)
plt.ylabel('Valor')
plt.ylim(0, 1)
plt.xticks(index, df_sorted['T√©cnica'], rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(frameon=True, loc='best')

# Mejoramos espaciado
plt.tight_layout()
plt.subplots_adjust(top=0.9, bottom=0.15)

# guardamos gr√°fico como archivo png
plt.savefig('graf_DTC_recall_por_tecnica_balaceo.png', dpi=300, bbox_inches='tight')
plt.show()

"""<font color=red size=4>El mejor modelo DecisionTree se obtuvo con RandomOverSampler</font>

<font color=red size=5>Intervalo de confianza del Recall para DecisionTree con RandomOverSamper</font>
"""

# revisemos el intervalo de confianza para el modelo balanceado con RandomOverSampler
# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separamos variables predictoras y objetivo
X = df.drop('Churn', axis=1)
y = df['Churn']

# Creamos pipeline con RandomOverSampler + modelo
pipeline = imbpipeline([
    ('oversample', RandomOverSampler(random_state=42)),
    ('modelo', DecisionTreeClassifier(random_state=42, max_depth=10))
])

# Validaci√≥n cruzada estratificada
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# M√©trica que queremos evaluar 'recall'
scores = cross_val_score(pipeline, X, y, cv=cv, scoring='recall')

# Calculamos media y desviaci√≥n est√°ndar
mean_score = np.mean(scores)
std_score = np.std(scores)

# Calculamos intervalo de confianza 95%
confidence = 0.95
z = stats.norm.ppf(1 - (1 - confidence)/2)
margin_error = z * std_score / np.sqrt(len(scores))
lower = mean_score - margin_error
upper = mean_score + margin_error

# Mostramos resultados
big_print(f"Recall promedio: {mean_score:.4f}",20,'red')
big_print(f"Intervalo de confianza del 95%: ({lower:.4f}, {upper:.4f})",20,'red')

"""<font color=red size=5>Curva ROC para DecisionTreeClassifier</font>"""

# mostramos curva ROC de pipeline
plot_roc_curve(pipeline, X_test, y_test, 'DecisionTreeClassifier con RandomOverSampler')

"""---


 <font color=red size=4>¬øQu√© representa?</font>



*   FPR (x): Proporci√≥n de negativos incorrectamente clasificados como positivos.
*   TPR (y): Proporci√≥n de positivos correctamente clasificados.
*   AUC (Area Under Curve): Cuanto m√°s cerca de 1, mejor es el modelo.





---

###<font color=blue size=5> DecisionTreeClassifier optimizaci√≥n de par√°metros</font>
"""

# Cargamos datos
df = pd.read_csv("df_codificado.csv")
X = df.drop("Churn", axis=1)
y = df["Churn"]

# Separamos en train y test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Definimos pipeline con oversampling
pipeline_dt = imbpipeline(steps=[
    ("oversample", RandomOverSampler(random_state=42)),
    ("model", DecisionTreeClassifier(random_state=42))
])

# Definimos grilla de hiperpar√°metros
param_grid = {
    "model__max_depth": [None, 5, 10, 15, 20],
    "model__min_samples_split": [2, 5, 10, 20],
    "model__min_samples_leaf": [1, 2, 4, 8],
    "model__criterion": ["gini", "entropy"]
}

# Scorer personalizado para recall de clase 1
recall_class1_scorer = make_scorer(recall_score, pos_label=1)

# B√∫squeda de hiperpar√°metros
grid_search = GridSearchCV(
    pipeline_dt,
    param_grid,
    scoring=recall_class1_scorer,  # optimiza recall de clase 1
    n_jobs=-1,
    cv=3
)

grid_search.fit(X_train, y_train)

# imprimimos mejores par√°metros
big_print("Mejores par√°metros encontrados DecisionTree:",20,'brown')
for param, value in grid_search.best_params_.items():
    print(f"{param}: \t{value}")

# Evaluamos en test
best_pipeline = grid_search.best_estimator_
y_pred = best_pipeline.predict(X_test)

# guardamos m√©tricas del modelo para informe fina
metricas_modelos.append(calcular_metricas('DecisionTree', y_test, y_pred))

# imprimimos reporte de clasificaci√≥n
big_print("\nReporte de clasificaci√≥n:",20,'brown')
print(classification_report(y_test, y_pred))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_pred, 'DecisionTree optimizado')

# mostramos curva ROC
plot_roc_curve(best_pipeline, X_test, y_test, 'DecisionTree optimizado')

"""###<font color=blue size=5>Pipeline DecisionTreeClassifier optimizado</font>"""

# Definimos pipeline con los par√°metros calculados previamente
pipeline_dt = grid_search.best_estimator_

# Entrenamos pipeline
pipeline_dt.fit(X_train, y_train)

# guardamos el pipeline optimizado
ruta_modelo = "pipeline_dt_optimizado.pkl"
joblib.dump(pipeline_dt, ruta_modelo)
big_print(f"Pipeline guardado en {ruta_modelo}",20)

# cargar el pipeline optimizado
pipeline_cargado = joblib.load(ruta_modelo)
big_print("Pipeline cargado correctamente.",20)

# ejecutamos el pipeline cargado en test
y_pred_cargado = pipeline_cargado.predict(X_test)

big_print("Reporte de clasificaci√≥n usando el pipeline cargado:",20,'brown')
print(classification_report(y_test, y_pred_cargado))

"""## <font color=blue size=5>CatBoost</font>

### <font color=blue size=5>CatBoost sin optimizaciones</font>
"""

# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separaci√≥n de variables dependiente e independientes
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separaci√≥n en Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# creamos modelo CatBoost
modelo = CatBoostClassifier(random_state=42,max_depth=10, verbose=0)

# entrenamos con conjunto de train
modelo.fit(X_train, y_train, verbose=False)

# revisemos las m√©tricas y la matriz de confusi√≥n
y_previsto = modelo.predict(X_test)

big_print("Reporte de Clasificaci√≥n CatBoost sin optimizaciones", 20,'firebrick')
print(classification_report(y_test, y_previsto))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_previsto, 'CatBoost sin optimizaciones')

"""###<font color=blue size=5> CatBosst con Validaci√≥n cruzada</font>"""

# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separamos variables predictoras (X) y variable objetivo (y)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separamos en conjunto de entrenamiento y prueba (stratify mantiene proporci√≥n de clases)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)

# creamos modelo CatBoost
modelo = CatBoostClassifier(random_state=42,max_depth=10, verbose=0)

# Validaci√≥n cruzada sobre el conjunto de entrenamiento
scoring = ['accuracy', 'precision', 'recall', 'f1']
cv_resultados = cross_validate(modelo, X_train, y_train, cv=5,
                               scoring=scoring, return_train_score=False)

# Mostramos resultados promedios de la validaci√≥n cruzada
big_print("CatBoost con Validaci√≥n cruzada",20,'brown')
for metrica in scoring:
    puntajes = cv_resultados[f'test_{metrica}']
    print(f"{metrica.capitalize():<10}: {puntajes.mean():.3f} ¬± {puntajes.std():.3f}")

# Entrenamos el modelo final con todos los datos de entrenamiento
modelo.fit(X_train, y_train, verbose=False)

# Predecimos con datos de test (jam√°s vistos por el modelo)
y_pred = modelo.predict(X_test)

# Reporte de clasificaci√≥n
big_print("Reporte de Clasificaci√≥n CatBoost con Validaci√≥n Cruzada", 20,'firebrick')
print(classification_report(y_test, y_pred))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_pred, 'CatBoost con Validaci√≥n cruzada')

"""###<font color=blue size=5> CatBosst con balanceo de clases</font>

---


Probaremos dos t√©cnicas


*   auto_class_weights='Balanced'
*   weight_ratio = n_class0 / n_class1

---


<font color=red size=4>Balanceo con auto_class_weights='Balanced'</font>
"""

# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separaci√≥n de variables dependiente e independientes
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separaci√≥n en Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# creamos modelo CatBoost con balanceo de clases
modelo = CatBoostClassifier(random_state=42,max_depth=10,
                          auto_class_weights='Balanced',
                          verbose=0)

# entrenamos con conjunto de train
modelo.fit(X_train, y_train, verbose=False)

# revisemos las m√©tricas y la matriz de confusi√≥n
y_previsto = modelo.predict(X_test)

big_print("Reporte de Clasificaci√≥n CatBoost class_weights Balanced", 20,'firebrick')
print(classification_report(y_test, y_previsto))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_previsto, 'CatBoost class_weights Balanced')

"""

---


<font color=red size=4>Balanceo con weight_ratio = n_class0 / n_class1</font>"""

# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separaci√≥n de variables dependiente e independientes
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separaci√≥n en Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# creamos modelo CatBoost con balanceo de clases definido seg√∫n ratio de clases
# Calculamos ratio exacto
n_class0 = sum(y_train == 0)
n_class1 = sum(y_train == 1)
weight_ratio = n_class0 / n_class1

modelo = CatBoostClassifier(
    class_weights=[1, weight_ratio],
    iterations=1000,
    early_stopping_rounds=50,
    eval_metric='Recall',
    random_state=42,
    verbose=0
)

modelo.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    use_best_model=True,
    verbose=False
)

# revisemos las m√©tricas y la matriz de confusi√≥n
y_previsto = modelo.predict(X_test)

big_print("Reporte de Clasificaci√≥n CatBoost weight_ratio calculado", 20,'firebrick')
print(classification_report(y_test, y_previsto))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_previsto, 'CatBoost weight_ratio calculado')

"""<font color=red size=4>El balanceo utilizando weight_ratio = n_class0 / n_class1 tiene resultados mucho mejores que utilizando auto_class_weights='Balanced'</font>


---

###<font color=blue size=5>CatBoost optimizaci√≥n de par√°metros</font>

---


<font color=red size=4>B√∫squeda Aleatoria (RandomizedSearchCV)</font>
"""

param_dist = {
    'depth': np.arange(4, 11),
    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],
    'l2_leaf_reg': np.logspace(-2, 2, 10),
    'border_count': [32, 64, 128, 256],
    'min_data_in_leaf': [1, 5, 10, 20],
    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']
}

modelo = CatBoostClassifier(
    class_weights=[1, weight_ratio],
    iterations=1000,
    early_stopping_rounds=50,
    random_state=42,
    verbose=0
)

search = RandomizedSearchCV(
    modelo,
    param_distributions=param_dist,
    n_iter=50,
    cv=5,
    scoring='recall',
    n_jobs=-1,
    random_state=42
)

search.fit(X_train, y_train, eval_set=[(X_test, y_test)])

big_print("Mejores par√°metros encontrados para CaatBoost:",20,'brown')

# imprimimos mejores par√°metros
for param, value in search.best_params_.items():
    print(f"{param}: {value}")

# Evaluar en test
mejor_modelo = search.best_estimator_
y_pred = mejor_modelo.predict(X_test)

# guardamos m√©tricas para informe final
metricas_modelos.append(calcular_metricas('CatBoost', y_test, y_pred))

# reporte de clasificaci√≥n
big_print("Reporte de clasificaci√≥n CatBoost optimizado",20,'brown')
print(classification_report(y_test, y_pred))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_pred, 'CatBoost optimizado')

# mostramos curva ROC de pipeline
plot_roc_curve(pipeline, X_test, y_test, 'CatBoost optimizado')

"""###<font color=blue size=5>Pipeline CatBoost optimizado</font>"""

# Definimos pipeline con los par√°metros calculados previamente
pipeline_cb = search.best_estimator_

# Entrenamos pipeline
pipeline_cb.fit(X_train, y_train, eval_set=[(X_test, y_test)])

# guardamos el pipeline optimizado usando try para manejo de error


ruta_modelo = "pipeline_CatBoost_optimizado.pkl"
joblib.dump(pipeline_cb, ruta_modelo)
big_print(f"Pipeline guardado en {ruta_modelo}",20,'brown')

# cargar el pipeline optimizado
pipeline_cargado = joblib.load(ruta_modelo)
big_print("Pipeline cargado correctamente.",20,'brown')

# ejecutamos el pipeline cargado en test
y_pred_cargado = pipeline_cargado.predict(X_test)

big_print("Reporte de clasificaci√≥n usando el pipeline cargado:",20,'brown')
print(classification_report(y_test, y_pred_cargado))

"""## <font color=blue size=5>RandomForestClassifier</font>

### <font color=blue size=5>RandomForestClassifier sin optimizaciones</font>
"""

# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separaci√≥n de variables dependiente e independientes
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separaci√≥n en Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# creamos modelo RandomForestClassifier
modelo = RandomForestClassifier(random_state=42,max_depth=10)

# entrenamos con conjunto de train
modelo.fit(X_train, y_train)

# revisemos las m√©tricas y la matriz de confusi√≥n
y_previsto = modelo.predict(X_test)

big_print("Reporte de Clasificaci√≥n RandomForest sin optimizaciones", 20,'firebrick')
print(classification_report(y_test, y_previsto))

#mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_previsto, 'RandomForest sin optimizaciones')

"""<font color=red size=4>
¬øQu√© significa esta matriz de confusi√≥n?</font>

1. que predijo correctamente 933 Churn No (Verdaderos Negativos TN) **90%**

2. que predijo err√≥neamente 102 Churn No como Churn S√≠ (Falsos Positivos FP)

3. predijo correctamente 195 Churn S√≠ (Verdaderos Positivos TP) **48%**

4. predijo err√≥neamente 179 Churn S√≠ como Churn No (Falsos Negativos FN)

Al igual que con DecisionTreeClassifier, este modelo es pr√°cticamente in√∫til para predecir correctamente los Churn S√≠ ya que solo identifica correctamente al 48% de ellos a pesar de que identifica correctamente a casi al 90% de los Churn No.

Este modelo no es √∫til porque nuestro objetivo es identificar lo mejor posible los clientes con Churn S√≠.

**Nuestro objetivo es encontrar un modelo donde el recall para Churn S√≠ sea el mayor posible.**


---

###<font color=blue size=5> RandomForestClassifier con Validaci√≥n cruzada</font>
"""

# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separamos variables predictoras (X) y variable objetivo (y)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Separamos en conjunto de entrenamiento y prueba (stratify mantiene proporci√≥n de clases)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)

# Creamos modelo RandomForest
modelo = RandomForestClassifier(random_state=42, max_depth=10)

# Validaci√≥n cruzada sobre el conjunto de entrenamiento
scoring = ['accuracy', 'precision', 'recall', 'f1']
cv_resultados = cross_validate(modelo, X_train, y_train, cv=5,
                               scoring=scoring, return_train_score=False)

# Mostramos resultados promedios de la validaci√≥n cruzada
big_print("RandomForest con Validaci√≥n cruzada",20,'brown')
for metrica in scoring:
    puntajes = cv_resultados[f'test_{metrica}']
    print(f"{metrica.capitalize():<10}: {puntajes.mean():.3f} ¬± {puntajes.std():.3f}")

# Entrenamos el modelo final con todos los datos de entrenamiento
modelo.fit(X_train, y_train)

# Predecimos con datos de test (jam√°s vistos por el modelo)
y_pred = modelo.predict(X_test)

# Reporte de clasificaci√≥n
big_print("Reporte de Clasificaci√≥n RandomForest con Validaci√≥n Cruzada", 20,'firebrick')
print(classification_report(y_test, y_pred))

# mostramos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_pred, 'RandomForest con Validaci√≥n cruzada')

"""<font color=red size=4>
¬øQu√© significa esta matriz de confusi√≥n de RandomForest con validaci√≥n cruzada?</font>

1. que predijo correctamente 933 Churn No (Verdaderos Negativos TN) **90%**

2. que predijo err√≥neamente 102 Churn No como Churn S√≠ (Falsos Positivos FP)

3. predijo correctamente 179 Churn S√≠ (Verdaderos Positivos TP) **48%**

4. predijo err√≥neamente 195 Churn S√≠ como Churn No (Falsos Negativos FN)

<font color=red size=4>
¬øQu√© significan las m√©tricas obtenidas con la validaci√≥n cruzada?</font>



1.   **RandomForestClassifier** entrenado con validaci√≥n cruzada (CV) muestra las mismas m√©tricas que el modelo entrenado sin CV (usando un solo train-test split), esto nos indica:

  - Estabilidad del modelo: El modelo es robusto ante diferentes particiones de datos.

  - No es sensible a variaciones aleatorias en la divisi√≥n entrenamiento/prueba.

2. Calidad de los datos: Los datos est√°n bien distribuidos y no hay sesgos fuertes o patrones ocultos que afecten el rendimiento seg√∫n c√≥mo se dividan.

3. El tama√±o del dataset es adecuado (no es demasiado peque√±o), lo que reduce la variabilidad en las m√©tricas entre diferentes splits.

4. Ausencia de overfitting significativo
El modelo generaliza bien: su rendimiento en datos no vistos es consistente, independientemente de c√≥mo se eval√∫e (CV vs. holdout simple).

###<font color=blue size=5> RandomForestClassifier con balanceo de clases</font>

---


<font color=red size=4>Probaremos las mismas cuatro t√©cnicas de balanceo que probamos con DecisionTreeClassifier m√°s la opci√≥n propia de RandomFores classweight</font>


*   RandomOverSampler
*   SMOTE
*   ADASYN
*   TomekLinks
*   classweight = 'balanced'



---
"""

# Cargamos datos
df = pd.read_csv("df_codificado.csv")

# Separamos variables
X = df.drop("Churn", axis=1)
y = df["Churn"]

# Separamos en train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Diccionario con t√©cnicas de muestreo
samplers = {
    "RandomOverSampler": RandomOverSampler(random_state=42),
    "SMOTE": SMOTE(random_state=42),
    "ADASYN": ADASYN(random_state=42),
    "TomekLinks": TomekLinks(),
    "classweight": "balanced"
}

# Almacenar resultados
resultados = []

# Entrenamiento y evaluaci√≥n por t√©cnica
def entrenar_y_evaluar(X_res, y_res, nombre):
    if nombre == "classweight":
        modelo = RandomForestClassifier(max_depth=10, class_weight='balanced', random_state=42)
    else:
      modelo = RandomForestClassifier(max_depth=10, random_state=42)

    modelo.fit(X_res, y_res)
    y_pred = modelo.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    resultados.append({
        "T√©cnica": nombre,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1
    })

    big_print(f"===== {nombre} =====",20,'brown')
    print(classification_report(y_test, y_pred))

    # mostramos matriz de confuci√≥n
    plot_confusion_matrix(y_test, y_pred, f"RandomForest {nombre}")

# Ejecutar para cada t√©cnica
for nombre, sampler in samplers.items():
    try:
        if nombre == "classweight":
          X_res, y_res = (X_train, y_train)
        else:
          X_res, y_res = sampler.fit_resample(X_train, y_train)
        entrenar_y_evaluar(X_res, y_res, nombre)
    except Exception as e:
        print(f"Error en {nombre}: {e}")

# Mostrar tabla comparativa
df_resultados = pd.DataFrame(resultados)
big_print("üìä M√©tricas RandomForest por T√©cnica de Balanceo:",20,'brown')
print(df_resultados.sort_values("Recall", ascending=False))

"""<font color=red size=5>Graficamos Recall y F1 para las cinco t√©cnicas de balanceo</font>"""

# Ordenar el DataFrame por Recall de mayor a menor
df_sorted = df_resultados.sort_values('Recall', ascending=False)

# Configuraci√≥n del gr√°fico
plt.figure(figsize=(8, 4), facecolor='lightyellow')
ax = plt.gca()
ax.set_facecolor('honeydew')

# Configuraci√≥n de barras
n = len(df_sorted)
bar_width = 0.35
index = np.arange(n)

# Creamos barras para Recall y F1-score
plt.bar(index - bar_width/2, df_sorted['Recall'], bar_width,
        color='skyblue', label='Recall')
plt.bar(index + bar_width/2, df_sorted['F1-score'], bar_width,
        color='lightgreen', label='F1-score')

# A√±adimos valores en las barras
for i, (rec, f1) in enumerate(zip(df_sorted['Recall'], df_sorted['F1-score'])):
    plt.text(i - bar_width/2, rec + 0.02, f"{rec:.3f}",
             ha='center', fontsize=9, color='darkblue')
    plt.text(i + bar_width/2, f1 + 0.02, f"{f1:.3f}",
             ha='center', fontsize=9, color='darkred')

# Personalizaci√≥n del gr√°fico
plt.title('RandomForest Recall y F1-Score por t√©cnica de balanceo', pad=20, fontsize=14)
plt.ylabel('Valor')
plt.ylim(0, 1)
plt.xticks(index, df_sorted['T√©cnica'], rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(frameon=True, loc='best')

# Mejorar espaciado
plt.tight_layout()
plt.subplots_adjust(top=0.9, bottom=0.15)
# guardamos gr√°fico como archivo png
plt.savefig('graf_RF_recall_por_tecnica_balaceo.png', dpi=300, bbox_inches='tight')
plt.show()

"""<font color=red size=5>Intervalo de confianza del Recall</font>"""

# revisemos el intervalo de confianza para el modelo balanceado con RandomOverSampler
# Cargamos el dataset
df = pd.read_csv('df_codificado.csv')

# Separamos variables predictoras y objetivo
X = df.drop('Churn', axis=1)
y = df['Churn']

# Creamos pipeline con RandomOverSampler + modelo
pipeline = imbpipeline([
    ('oversample', RandomOverSampler(random_state=42)),
    ('modelo', RandomForestClassifier(random_state=42, max_depth=10))
])

# Validaci√≥n cruzada estratificada
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# M√©trica que quieremos evaluar 'recall'
scores = cross_val_score(pipeline, X, y, cv=cv, scoring='recall')

# Calcular media y desviaci√≥n est√°ndar
mean_score = np.mean(scores)
std_score = np.std(scores)

# Calcular intervalo de confianza 95%
confidence = 0.95
z = stats.norm.ppf(1 - (1 - confidence)/2)
margin_error = z * std_score / np.sqrt(len(scores))
lower = mean_score - margin_error
upper = mean_score + margin_error

# Mostrar resultados
big_print(f"Recall promedio: {mean_score:.4f}",20,'red')
big_print(f"Intervalo de confianza del 95%: ({lower:.4f}, {upper:.4f})",20,'red')

"""<font color=red size=5>Curva ROC para RandomForestClassifier</font>"""

# mostramos curba ROC con el pipeline
titulo = "RandomForest con RandomOverSampler"
plot_roc_curve(pipeline, X_test, y_test,titulo)

"""---


 <font color=red size=4>¬øQu√© representa?</font>



*   FPR (x): Proporci√≥n de negativos incorrectamente clasificados como positivos.
*   TPR (y): Proporci√≥n de positivos correctamente clasificados.
*   AUC (Area Under Curve): Cuanto m√°s cerca de 1, mejor es el modelo.





---

###<font color=blue size=5>RandomForest  Optimizaci√≥n de par√°metros</font>
"""

# Cargar los datos
df = pd.read_csv("df_codificado.csv")
X = df.drop("Churn", axis=1)
y = df["Churn"]

# Separar en Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Pipeline con balanceo y modelo
pipeline = imbpipeline([
    ('oversample', RandomOverSampler(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42))
])

# Hiperpar√°metros a optimizar
param_grid = {
    'clf__n_estimators': [100, 200, 300],
    'clf__max_depth': [None, 10, 20, 30],
    'clf__min_samples_split': [2, 5, 10],
    'clf__min_samples_leaf': [1, 2, 4],
    'clf__max_features': ['sqrt', 'log2'],
    'clf__class_weight': ['balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]
}

# Scoring personalizado: solo el recall de la clase positiva
recall_pos_scorer = make_scorer(recall_score, pos_label=1)

# B√∫squeda aleatoria de par√°metros
random_search = RandomizedSearchCV(
    pipeline,
    param_distributions=param_grid,
    n_iter=30,
    cv=5,
    scoring=recall_pos_scorer,  # Enfocado en recall clase 1
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Entrenamiento
random_search.fit(X_train, y_train)

big_print("Mejores par√°metros encontrados para RandomForest:",20,'brown')

# imprimimos mejores par√°metros
for param, value in random_search.best_params_.items():
    print(f"{param}: \t{value}")

# Evaluar en test
mejor_modelo = random_search.best_estimator_
y_pred = mejor_modelo.predict(X_test)

# guardamos m√©tricas para informe final
metricas_modelos.append(calcular_metricas('RandomForest', y_test, y_pred))

big_print("Reporte de clasificaci√≥n RandomForest optimizado",20,'brown')
print(classification_report(y_test, y_pred))

# desplegamos matriz de confusi√≥n
plot_confusion_matrix(y_test, y_pred, 'RandomForest optimizado')

"""<font color=blue size=5>Intervalo de confianza de Recall para RandomForest optimizado</font>"""

# Mejor modelo ya entrenado
mejor_modelo = random_search.best_estimator_

# N√∫mero de repeticiones del bootstrap
n_iterations = 1000
recalls = []

# Probabilidades para fijar la semilla y asegurar reproducibilidad
rng = np.random.default_rng(seed=42)

# Bootstrap sobre los datos de test
for _ in range(n_iterations):
    # Re-muestreo con reemplazo
    indices = rng.integers(0, len(X_test), len(X_test))
    X_resampled = X_test.iloc[indices]
    y_resampled = y_test.iloc[indices]

    # Predicci√≥n
    y_pred = mejor_modelo.predict(X_resampled)

    # Calcular recall clase 1
    recalls.append(recall_score(y_resampled, y_pred, pos_label=1))

# Intervalo de confianza al 95%
lower = np.percentile(recalls, 2.5)
upper = np.percentile(recalls, 97.5)

big_print(f"Recall medio (clase 1): {np.mean(recalls):.3f}",20,'brown')
big_print(f"IC 95% para Recall clase 1: ({lower:.3f}, {upper:.3f})",20,'brown')

"""<font color=red size=5>Curva ROC para DecisionTreeClassifier optimizado</font>"""

# Mejor modelo optimizado encontrado por RandomizedSearchCV
mejor_modelo = random_search.best_estimator_

# desplegamos curva ROC
titulo = "RF optimizado"
plot_roc_curve(mejor_modelo, X_test, y_test,titulo)

"""### <font color=blue size=5>Pipeline RandomForest optimizado</font>

---
<font color=red size=4>
El modelo RandomForestClassifier optimizado predijo correctamente al 90% de los Churn S√≠.

Predijo correctamente al 61% de los Churn No.</font>


<font color=red size=4>
Este es un modelo aceptable</font>

---
"""

# Par√°metros optimizados que obtuvimos con RandomizedSearchCV

parametros_optimizados = {
    'n_estimators': 100,
    'max_depth': 10,
    'min_samples_split': 5,
    'min_samples_leaf': 4,
    'max_features': 'sqrt',
    'bootstrap': True,
    'class_weight': {0: 1, 1: 3},
    'random_state': 42
}

modelo_optimizado = RandomForestClassifier(**parametros_optimizados)
modelo_optimizado.fit(X_train, y_train)

# Creamos el pipeline
pipeline_optimizado = imbpipeline(steps=[
    ('oversample', RandomOverSampler(random_state=42)),
    ('rf', RandomForestClassifier(**parametros_optimizados))
])

# Entrenamos el pipeline en los datos de entrenamiento
pipeline_optimizado.fit(X_train, y_train)

# Evaluamos en el conjunto de prueba
y_pred = pipeline_optimizado.predict(X_test)

# guardamos el pipeline optimizado
ruta_modelo = "pipeline_RandomForest_optimizado.pkl"
joblib.dump(pipeline_optimizado, ruta_modelo)
big_print(f"Pipeline guardado en {ruta_modelo}",20,'brown')

# cargamos pipeline optimizado
pipeline_cargado = joblib.load(ruta_modelo)
big_print("Pipeline cargado correctamente.",20,'brown')

# ejecutamos pipeline sobre test
y_pred_cargado = pipeline_cargado.predict(X_test)

big_print("Reporte de clasificaci√≥n usando el pipeline cargado:",20,'brown')
print(classification_report(y_test, y_pred_cargado))

"""### <font color=blue size=5>Feature importance</font>"""

# Accedemos al modelo dentro del pipeline usando su nombre ('rf')
modelo_final = pipeline_optimizado.named_steps['rf']

# Extraemos las importancias del modelo
importancias = modelo_final.feature_importances_

# Creamos un DataFrame para visualizaci√≥n
df_importancias = pd.DataFrame({
    'Caracter√≠stica': X_train.columns,
    'Importancia': importancias
})

# Ordenamos de mayor a menor importancia
df_importancias = df_importancias.sort_values('Importancia', ascending=True)

# seleccionamos solo las caracter√≠sticas con una importancia mayor al 1%
df_importancias = df_importancias[df_importancias['Importancia'] > 0.02]

# Mostrar las 10 caracter√≠sticas m√°s importantes
# print(df_importancias.head(10))

# graficamos importancias en barras horizontales en orden descendentes
plt.figure(figsize=(7, 5), facecolor='lightyellow')
ax = plt.gca()
ax.set_facecolor('honeydew')
plt.title('Top 10 de caracter√≠sticas RandomForest optimizado', fontsize=14, loc='left')
plt.xlabel('Importancia', color='firebrick', fontsize=12)
plt.ylabel('Caracter√≠stica', color='firebrick', fontsize=12)

plt.barh(df_importancias['Caracter√≠stica'], df_importancias['Importancia'])

# guardamos gr√°fico como archivo png
plt.savefig('graf_RF_importancias.png', dpi=300, bbox_inches='tight')
plt.show()

"""## <font color=blue size=5>Resumen de m√©tricas de modelos</font>"""

# Crear DataFrame
metricas_df = pd.DataFrame(metricas_modelos)

# Formatear las m√©tricas para mejor visualizaci√≥n
metricas_df_formateado = metricas_df.copy()
for col in ['Accuracy', 'Precision_Clase_1', 'Recall_Clase_1', 'F1-Score_Clase_1']:
    metricas_df_formateado[col] = metricas_df_formateado[col].map('{:.2%}'.format)

# definimos 'Modelo' como index
metricas_df_formateado.set_index('Modelo', inplace=True)
# Mostrar tabla comparativa
big_print("Tabla Comparativa de Modelos:",20,'brown')
display(metricas_df_formateado)

# Ordenar por Recall para mejor visualizaci√≥n
metricas_df = metricas_df.sort_values('Recall_Clase_1', ascending=False)

# Configuraci√≥n del gr√°fico
plt.figure(figsize=(8, 5), facecolor='lightyellow')
ax = plt.subplot(facecolor='honeydew')

# Crear colores con gradiente
colors = plt.cm.Blues(np.linspace(0.5, 0.9, len(metricas_df)))

# Crear gr√°fico de barras
bars = ax.bar(metricas_df['Modelo'],
              metricas_df['Recall_Clase_1'] * 100,
              color=colors,
              edgecolor='#333333',
              width=0.5)

# A√±adir valores en las barras
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2.,
            height + 0.2,  # Ajuste menor de posici√≥n
            f'{height:.1f}%',
            ha='center',
            va='bottom',
            fontsize=12,
            fontweight='bold')

# Personalizaci√≥n
plt.title('Comparaci√≥n de Recall para Clase 1 (Churn=Yes)',
          fontsize=16, pad=20, fontweight='bold', color='#333333')
plt.xlabel('')
plt.ylabel('Recall (%)', fontsize=12, labelpad=10, color='#333333')

# AJUSTE PRINCIPAL: Eje Y comienza en 60%
y_min = 60
y_max = metricas_df['Recall_Clase_1'].max() * 100 * 1.15  # 15% m√°s de espacio arriba
plt.ylim(y_min, y_max)

# A√±adir l√≠nea de referencia
avg_recall = metricas_df['Recall_Clase_1'].mean() * 100
plt.axhline(y=avg_recall,
            color='#e74c3c',
            linestyle='--',
            linewidth=1.5,
            label=f'Recall Promedio: {avg_recall:.1f}%')

# Decoraci√≥n
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_color('#cccccc')
ax.spines['bottom'].set_color('#cccccc')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.legend(loc='upper right')

# A√±adir anotaci√≥n sobre la escala
plt.figtext(0.98, 0.02,
            f"Eje Y: {y_min}%-{y_max:.0f}%",
            ha='right', fontsize=9, color='#666666')
plt.grid(False)
plt.tight_layout()
plt.subplots_adjust(bottom=0.15)  # M√°s espacio abajo

# guardamos gr√°fico como archivo png
plt.savefig('graf_recall_por_modelo.png', dpi=300, bbox_inches='tight')
plt.show()



"""## <font color=blue size=5>Pipeline completo de Random Forest con preprocesamiento</font>

<font color=red size=4>Este Pipeline se crear√° para recibir los datos en su formato original, e incluir√° todas las transformaciones realizadas para crear el modelo.</font>
"""

# Leemos archivo /content/df_normalizado.csv
df = pd.read_csv('/content/df_normalizado.csv')

# Separamos las variables dependiente e independientes
X = df.drop('Churn', axis=1)
y = df['Churn']

# Dividimos train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Transformador para eliminar columnas
class ColumnDropper(BaseEstimator, TransformerMixin):
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop
        self.columns_ = None  # Inicializamos como None

    def fit(self, X, y=None):
        # Identificar las columnas que existen y queremos eliminar
        self.columns_ = [col for col in self.columns_to_drop if col in X.columns]
        return self

    def transform(self, X):
        # Si no se ha llamado a fit, intentamos determinar las columnas
        if self.columns_ is None:
            self.columns_ = [col for col in self.columns_to_drop if col in X.columns]
        return X.drop(columns=self.columns_, errors='ignore')

# Transformador para valores "Yes"
class YesToBinaryTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.columns_to_transform = []  # Inicializamos como lista vac√≠a

    def fit(self, X, y=None):
        # Identificar columnas que contienen 'Yes'
        for col in X.columns:
            if X[col].dtype == 'object':
                unique_vals = X[col].dropna().unique()
                if 'Yes' in unique_vals:
                    self.columns_to_transform.append(col)
        return self

    def transform(self, X):
        X_copy = X.copy()
        for col in self.columns_to_transform:
            if col in X_copy.columns:
                X_copy[col] = X_copy[col].apply(lambda x: 1 if str(x) == 'Yes' else 0)
        return X_copy

# Par√°metros optimizados
parametros_optimizados = {
    'n_estimators': 100,
    'max_depth': 10,
    'min_samples_split': 5,
    'min_samples_leaf': 4,
    'max_features': 'sqrt',
    'bootstrap': True,
    'class_weight': {'No': 1, 'Yes': 3},
    'random_state': 42
}

# Columnas para one-hot encoding
categorical_cols = ['gender', 'InternetService', 'Contract', 'PaymentMethod']

# Preprocesador
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)
    ],
    remainder='passthrough'
)

# Creamos pipeline de entrenamiento con todo el preprocesamiento
pipeline_entrenamiento = imbpipeline(steps=[
    ('drop_columns', ColumnDropper(columns_to_drop=['customerID', 'Charges.Monthly'])),
    ('yes_to_binary', YesToBinaryTransformer()),
    ('preprocessor', preprocessor),
    ('oversample', RandomOverSampler(random_state=42)),
    ('rf', RandomForestClassifier(**parametros_optimizados))
])

# Entrenamos el pipeline con todos los datos porque ya no necesitamos nuevas validaciones de test
pipeline_entrenamiento.fit(X, y)

# Guardamos el pipeline entrenado
ruta_modelo = "pipeline_final_RandomForest_entrenamiento.pkl"
joblib.dump(pipeline_entrenamiento, ruta_modelo)
big_print("Pipeline final RandomForest de entrenamiento guardado.",20,'brown')

# Creamos pipeline de producci√≥n a partir del pipeline de entrenamiento
# Cargamos el pipeline entrenado
pipeline_cargado = joblib.load(ruta_modelo)
big_print("Pipeline cargado correctamente.",20,'brown')

# Omitimos el paso de oversampling en producci√≥n porque no aplica
steps_sin_oversample = [
    (name, step) for name, step in pipeline_cargado.steps if name != 'oversample'
]

# Creamos pipeline de producci√≥n sin oversampling
pipeline_produccion = imbpipeline(steps=steps_sin_oversample)

# Verificaci√≥n antes de guardar
for name, step in pipeline_produccion.steps:
    if isinstance(step, RandomOverSampler):
        raise ValueError("Error: El pipeline de producci√≥n contiene un RandomOverSampler. Revisa la configuraci√≥n.")
big_print("Verificaci√≥n OK: No hay RandomOverSampler en producci√≥n.",20,'brown')

# Guardamos pipeline de producci√≥n
ruta_modelo = "pipeline_final_RandomForest_produccion.pkl"
joblib.dump(pipeline_produccion, ruta_modelo)
big_print("Pipeline RandomForest de producci√≥n guardado sin oversampling.",20,'brown')

# Creamos pipeline de entrenamiento con todo el preprocesamiento
pipeline_entrenamiento = imbpipeline(steps=[
    ('drop_columns', ColumnDropper(columns_to_drop=['customerID', 'Charges.Monthly'])),
    ('yes_to_binary', YesToBinaryTransformer()),
    ('preprocessor', preprocessor),
    ('oversample', RandomOverSampler(random_state=42)),
    ('rf', RandomForestClassifier(**parametros_optimizados))
])

# Entrenamos el pipeline con todos los datos porque ya no necesitamos nuevas validaciones de test
pipeline_entrenamiento.fit(X, y)

# Guardamos el pipeline entrenado
ruta_modelo = "pipeline_final_RandomForest_entrenamiento.pkl"
joblib.dump(pipeline_entrenamiento, ruta_modelo)
big_print("Pipeline final RandomForest de entrenamiento guardado.",20,'brown')

# Creamos pipeline de producci√≥n a partir del pipeline de entrenamiento
# Cargamos el pipeline entrenado
pipeline_cargado = joblib.load(ruta_modelo)
big_print("Pipeline cargado correctamente.",20,'brown')

# Omitimos el paso de oversampling en producci√≥n porque no aplica
steps_sin_oversample = [
    (name, step) for name, step in pipeline_cargado.steps if name != 'oversample'
]

# Creamos pipeline de producci√≥n sin oversampling
pipeline_produccion = imbpipeline(steps=steps_sin_oversample)

# Verificaci√≥n antes de guardar
for name, step in pipeline_produccion.steps:
    if isinstance(step, RandomOverSampler):
        raise ValueError("Error: El pipeline de producci√≥n contiene un RandomOverSampler. Revisa la configuraci√≥n.")
big_print("Verificaci√≥n OK: No hay RandomOverSampler en producci√≥n.",20,'brown')

# Guardamos pipeline de producci√≥n
ruta_modelo = "pipeline_final_RandomForest_produccion.pkl"
joblib.dump(pipeline_produccion, ruta_modelo)
big_print("Pipeline RandomForest de producci√≥n guardado sin oversampling.",20,'brown')

"""## <font color=blue size=5>Pruebas de pipeline de producci√≥n con datos nuevos</font>

### <font color=blue size=4>Prueba con registro individual</font>
"""

# Predecimos sobre un registro de nuevos datos
# creaci√≥n de un registro nuevo para aplicar modelo
registro = generar_registro_cliente()

# Convertimos a DataFrame para usar el modelo
nuevos_datos = pd.DataFrame([registro])

# Cargamos el pipeline de producci√≥n entrenado
ruta_modelo = "pipeline_final_RandomForest_produccion.pkl"
pipeline_final = joblib.load(ruta_modelo)

big_print("Nuevos datos",20,'brown')
print(nuevos_datos.T)

prediccion = pipeline_final.predict(nuevos_datos)
probabilidades = pipeline_final.predict_proba(nuevos_datos)

# Extraemos el √≠ndice de la clase 'Yes'
idx_yes = np.where(pipeline_final.classes_ == 'Yes')[0][0]
idx_no = np.where(pipeline_final.classes_ == 'No')[0][0]

proba = pipeline_final.predict_proba(nuevos_datos)[0]

# Determinamos Churn S√≠ o No
if proba[idx_yes] >= proba[idx_no]:
    prediccion_final = 'S√≠'
else:
    prediccion_final = 'No'

# Imprimimos resultados
big_print(f"¬øAbandonar√° el cliene {nuevos_datos.customerID.values}?:   {prediccion_final}",20,'brown')
big_print(f'Probabilidad de Churn S√≠: {proba[idx_yes]*100:.0f}%',20,'brown')
big_print(f'Probabilidad de Churn No: {proba[idx_no]*100:.0f}%',20,'brown')

"""### <font color=blue size=4>Prueba con conjunto de 1000 registros nuevos</font>"""

# Uso en producci√≥n con dataframe de 1000 registros nuevos
# Cargamos el pipeline de producci√≥n entrenado
ruta_modelo = "pipeline_final_RandomForest_produccion.pkl"
pipeline_final = joblib.load(ruta_modelo)

# generamos dataframe con 1000 nuevos datos usando la funci√≥n generar_registro_cliente
nuevos_datos = pd.DataFrame([generar_registro_cliente() for _ in range(1000)])

# predecimos sobre nuevos_datos
prediccion = pipeline_final.predict(nuevos_datos)
probabilidades = pipeline_final.predict_proba(nuevos_datos)

# Sacar el √≠ndice de la clase 'Yes'
idx_yes = np.where(pipeline_final.classes_ == 'Yes')[0][0]
idx_no = np.where(pipeline_final.classes_ == 'No')[0][0]

# agregamos a nuevos_datos columnas con probabilidades de S√≠ y No y conclusi√≥n dependiendo del valor mayor
nuevos_datos['Prob Churn S√≠'] = (probabilidades[:, idx_yes] * 100).round(0)
nuevos_datos['Prob Churn No'] = (probabilidades[:, idx_no] * 100).round(0)

nuevos_datos['Conclusi√≥n'] = np.where(probabilidades[:, idx_yes] > probabilidades[:, idx_no], 'Abandona', 'No abandona')

# asignamos customerID como √≠ndice
nuevos_datos.set_index('customerID', inplace=True)

# mostramos 'customerID', 'gender', 'Partner', 'Dependents', 'Contract', 'tenure', 'Prob Churn S√≠','Prob Churn No','Conclusi√≥n'
big_print("Probabilidades de Churn para nuevos datos",20,'brown')
nuevos_datos[['gender', 'Partner', 'Dependents', 'Contract', 'tenure', 'Prob Churn S√≠','Prob Churn No','Conclusi√≥n']].head(10)